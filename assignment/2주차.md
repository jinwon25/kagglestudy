# 2주차 Porto Seguro’s Safe Driver Prediction 캐글 필사

## 참고 노트북

[Porto Seguro Tutorial: end-to-end ensemble - Yifan Xie](https://www.kaggle.com/code/yifanxie/porto-seguro-tutorial-end-to-end-ensemble#4.-Level-2-ensemble)


## 내용 정리

### 스태킹(Stacking)이란?

스태킹은 여러 개의 개별 모델들을 학습시킨 후, 그들의 예측 결과를 새로운 데이터로 삼아 **메타 모델**을 훈련시키는 방법

- **개별 모델들:** 서로 다른 알고리즘을 사용하는 다양한 모델
- **메타 모델:** 개별 모델들의 예측 결과를 입력으로 받아 최종 예측을 수행하는 모델

![스크린샷](../image/screenshot4.png)

---
### 본 노트북에서의 스태킹 흐름

- **Level 1:** 다양한 개별 모델(Base Models)들의 예측 결과 생성 (OOF 방식)
- **Level 2:** Level 1 결과를 입력으로 받아 또 다른 모델을 학습 (Meta-Model)
- **Level 3:** Level 2 결과를 다시 입력으로 하여 최종 예측 모델 학습

#### ✅ Level 1: Base Models & OOF 예측 생성

**주요 모델**
- Random Forest
- Extra Trees
- Logistic Regression
- Bernoulli Naive Bayes
- XGBoost
- LightGBM

**핵심: OOF(Out-of-Fold) 예측**
- StratifiedKFold로 훈련 데이터를 K개로 나눔
- 각 모델은 K개의 폴드로 나눠 학습 및 검증
- 검증에 사용되지 않은 Fold로 예측값을 생성 → train_pred
- 테스트셋은 각 Fold에서 예측한 결과의 평균 → test_pred
```PYTHON
rf_train_pred, rf_test_pred = cross_validate_sklearn(rf, x_train, y_train, x_test, kf)
```
=> 이러한 방식으로 각 모델별로 예측값을 DataFrame으로 저장

---
#### ✅ Level 2: 메타 모델 학습

**Level 1 예측 결과들을 Feature로 구성**
```PYTHON
rf_train_pred, rf_test_pred = cross_validate_sklearn(rf, x_train, y_train, x_test, kf)
```

**Level 2 메타 모델**
- Cross-validation을 통해 다시 OOF 예측 수행
- 이 때 AUC 기반 예측이기 때문에 rank를 사용하는 것이 성능 향상에 도움이 됨
```PYTHON
xgb_lv2_outcomes = cross_validate_xgb(xgb_params, lv1_train_df, y_train, lv1_test_df, kf, use_rank=True)
```
=> 이렇게 얻은 결과는 레벨2 예측값이 됨

---
#### ✅ Level 3: 최종 스태킹

**Level 2 결과를 다시 feature로 구성**
```PYTHON
lv2_train = pd.DataFrame({
    'rf_lv2': rf_lv2_train_pred,
    'logit_lv2': logit_lv2_train_pred,
    'xgb_lv2': xgb_lv2_train_pred,
    'lgb_lv2': lgb_lv2_train_pred
})
```

**최종 모델 학습 (XGBoost & Logistic Regression)**
```PYTHON
xgb_lv3_outcomes = cross_validate_xgb(xgb_lv3_params, lv2_train, y_train, lv2_test, kf, use_rank=True)
logit_lv3_outcomes = cross_validate_sklearn(logit_lv3, lv2_train, y_train, lv2_test, kf, scale=True)
```

---
#### ✅ 최종 예측 및 앙상블
- 두 개의 Level 3 모델 예측 결과를 단순 평균으로 앙상블
```PYTHON
submission['target'] = 0.5 * logit_lv3_test_pred + 0.5 * xgb_lv3_test_pred
```

---
#### 📌 핵심 요약

| 단계      | 주요 모델                                | 목적           | 선택 이유 |
|-----------|-------------------------------------------|----------------|-----------|
| **Level 1** | RF, ET, Logit, NB, XGB, LGB               | 다양성 확보    | 서로 다른 패턴을 가진 예측값 생성 |
| **Level 2** | XGB, LGB, RF, Logit                       | 조합 최적화    | OOF 예측을 메타 모델에 넣어 성능 향상 |
| **Level 3** | XGB, Logit                                | 최종 조합      | 강력한 모델과 단순한 모델을 평균 앙상블 |
